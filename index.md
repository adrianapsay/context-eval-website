---
layout: default
title: "ContextEval: Evaluating LLM Agent Context Policies for ML Experiment Design"
---

# ContextEval

**Evaluating LLM Agent Context Policies for ML Experiment Design**

*Hikaru Isayama, Adrian Apsay, Julia Jung, Narasimhan Raghavan, Ryan Lingo*

UCSD DSC Capstone — 99P Labs / HRI & HDSI

---

## Abstract

---

## Introduction

### Motivation

### Research Questions

---

## Related Works

### LLM-Based Agents for ML Experimentation

### Prompting, Memory, and Context in LLM Agents

### Agent Benchmarks and Evaluation Frameworks

---

## Methodology

### Problem Setting

### ContextEval Framework

### Context Policies

### Experimental Setup

---

## Results

### Main Findings

### Context Axis Ablations

### Interaction Effects Between Axes

### Cost–Performance Tradeoffs

---

## Analysis & Discussion

### When Does More Context Help or Hurt?

### Task-Dependent Patterns

### Implications for Agent Design

### Limitations

---

## Conclusion

### Summary

### Future Work

---

## Code & Resources

- **Code:** [github.com/juliamsjung/context-eval](https://github.com/juliamsjung/context-eval)
